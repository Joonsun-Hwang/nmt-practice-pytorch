nohup: ignoring input
/home/joonsun/nmt-practice-pytorch/raw_data
[*] Downloading from http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz to training-parallel-nc-v13.tgz.
training-parallel-nc-v13.tgz: 0.00B [00:00, ?B/s]training-parallel-nc-v13.tgz:   0%|          | 8.19k/113M [00:01<5:28:20, 5.74kB/s]training-parallel-nc-v13.tgz:   0%|          | 16.4k/113M [00:01<4:08:52, 7.58kB/s]training-parallel-nc-v13.tgz:   0%|          | 49.2k/113M [00:01<2:58:55, 10.5kB/s]training-parallel-nc-v13.tgz:   0%|          | 106k/113M [00:02<2:07:56, 14.7kB/s] training-parallel-nc-v13.tgz:   0%|          | 221k/113M [00:02<1:30:49, 20.7kB/s]training-parallel-nc-v13.tgz:   0%|          | 426k/113M [00:02<1:04:13, 29.3kB/s]training-parallel-nc-v13.tgz:   1%|          | 836k/113M [00:03<45:10, 41.4kB/s]  training-parallel-nc-v13.tgz:   1%|▏         | 1.67M/113M [00:03<31:33, 58.9kB/s]training-parallel-nc-v13.tgz:   3%|▎         | 3.34M/113M [00:03<21:51, 83.7kB/s]training-parallel-nc-v13.tgz:   6%|▌         | 6.49M/113M [00:03<14:54, 119kB/s] training-parallel-nc-v13.tgz:   8%|▊         | 9.19M/113M [00:04<10:13, 169kB/s]training-parallel-nc-v13.tgz:  11%|█         | 12.3M/113M [00:04<06:59, 240kB/s]training-parallel-nc-v13.tgz:  13%|█▎        | 14.6M/113M [00:04<04:50, 339kB/s]training-parallel-nc-v13.tgz:  16%|█▌        | 17.7M/113M [00:05<03:19, 478kB/s]training-parallel-nc-v13.tgz:  18%|█▊        | 20.9M/113M [00:05<02:17, 671kB/s]training-parallel-nc-v13.tgz:  21%|██        | 23.7M/113M [00:05<01:35, 933kB/s]training-parallel-nc-v13.tgz:  24%|██▎       | 26.8M/113M [00:05<01:07, 1.29MB/s]training-parallel-nc-v13.tgz:  26%|██▋       | 29.8M/113M [00:06<00:47, 1.74MB/s]training-parallel-nc-v13.tgz:  29%|██▉       | 32.8M/113M [00:06<00:34, 2.32MB/s]training-parallel-nc-v13.tgz:  32%|███▏      | 35.9M/113M [00:06<00:25, 3.05MB/s]training-parallel-nc-v13.tgz:  35%|███▍      | 39.0M/113M [00:07<00:18, 3.91MB/s]training-parallel-nc-v13.tgz:  37%|███▋      | 42.2M/113M [00:07<00:14, 4.87MB/s]training-parallel-nc-v13.tgz:  40%|███▉      | 45.1M/113M [00:07<00:11, 5.76MB/s]training-parallel-nc-v13.tgz:  43%|████▎     | 48.1M/113M [00:07<00:09, 6.68MB/s]training-parallel-nc-v13.tgz:  45%|████▌     | 51.0M/113M [00:08<00:08, 7.48MB/s]training-parallel-nc-v13.tgz:  48%|████▊     | 54.0M/113M [00:08<00:07, 8.19MB/s]training-parallel-nc-v13.tgz:  50%|█████     | 57.0M/113M [00:08<00:06, 8.81MB/s]training-parallel-nc-v13.tgz:  53%|█████▎    | 59.8M/113M [00:08<00:05, 9.21MB/s]training-parallel-nc-v13.tgz:  55%|█████▌    | 62.8M/113M [00:09<00:05, 9.56MB/s]training-parallel-nc-v13.tgz:  58%|█████▊    | 65.7M/113M [00:09<00:04, 9.83MB/s]training-parallel-nc-v13.tgz:  61%|██████    | 68.6M/113M [00:09<00:04, 9.96MB/s]training-parallel-nc-v13.tgz:  63%|██████▎   | 71.5M/113M [00:10<00:04, 10.1MB/s]training-parallel-nc-v13.tgz:  66%|██████▌   | 74.5M/113M [00:10<00:03, 10.2MB/s]training-parallel-nc-v13.tgz:  68%|██████▊   | 77.3M/113M [00:10<00:03, 10.2MB/s]training-parallel-nc-v13.tgz:  71%|███████   | 80.4M/113M [00:10<00:03, 10.4MB/s]training-parallel-nc-v13.tgz:  74%|███████▎  | 83.4M/113M [00:11<00:02, 10.5MB/s]training-parallel-nc-v13.tgz:  76%|███████▋  | 86.3M/113M [00:11<00:02, 10.4MB/s]training-parallel-nc-v13.tgz:  79%|███████▉  | 89.2M/113M [00:11<00:02, 10.4MB/s]training-parallel-nc-v13.tgz:  81%|████████▏ | 92.2M/113M [00:12<00:02, 10.5MB/s]training-parallel-nc-v13.tgz:  84%|████████▍ | 95.3M/113M [00:12<00:01, 10.7MB/s]training-parallel-nc-v13.tgz:  87%|████████▋ | 98.5M/113M [00:12<00:01, 10.9MB/s]training-parallel-nc-v13.tgz:  90%|████████▉ | 102M/113M [00:12<00:01, 11.0MB/s] training-parallel-nc-v13.tgz:  92%|█████████▏| 104M/113M [00:13<00:00, 10.6MB/s]training-parallel-nc-v13.tgz:  95%|█████████▍| 107M/113M [00:13<00:00, 10.7MB/s]training-parallel-nc-v13.tgz:  98%|█████████▊| 110M/113M [00:13<00:00, 10.6MB/s]training-parallel-nc-v13.tgz: 113MB [00:13, 8.20MB/s]                           
[*] Extracting training-parallel-nc-v13.tgz.
/home/joonsun/nmt-practice-pytorch/raw_data
[*] Downloading from http://data.statmt.org/wmt18/translation-task/dev.tgz to dev.tgz.
dev.tgz: 0.00B [00:00, ?B/s]dev.tgz:   0%|          | 8.19k/32.4M [00:00<37:00, 14.6kB/s]dev.tgz:   0%|          | 16.4k/32.4M [00:00<31:25, 17.2kB/s]dev.tgz:   0%|          | 49.2k/32.4M [00:01<23:21, 23.1kB/s]dev.tgz:   0%|          | 106k/32.4M [00:01<17:06, 31.5kB/s] dev.tgz:   1%|          | 221k/32.4M [00:01<12:19, 43.5kB/s]dev.tgz:   1%|▏         | 451k/32.4M [00:01<08:45, 60.8kB/s]dev.tgz:   3%|▎         | 918k/32.4M [00:02<06:08, 85.5kB/s]dev.tgz:   6%|▌         | 1.84M/32.4M [00:02<04:12, 121kB/s]dev.tgz:  11%|█▏        | 3.69M/32.4M [00:02<02:47, 171kB/s]dev.tgz:  18%|█▊        | 5.96M/32.4M [00:03<01:49, 242kB/s]dev.tgz:  20%|██        | 6.61M/32.4M [00:03<01:17, 333kB/s]dev.tgz:  28%|██▊       | 9.10M/32.4M [00:03<00:49, 468kB/s]dev.tgz:  38%|███▊      | 12.2M/32.4M [00:03<00:30, 657kB/s]dev.tgz:  45%|████▍     | 14.5M/32.4M [00:04<00:19, 902kB/s]dev.tgz:  54%|█████▍    | 17.5M/32.4M [00:04<00:11, 1.24MB/s]dev.tgz:  60%|██████    | 19.5M/32.4M [00:04<00:07, 1.65MB/s]dev.tgz:  67%|██████▋   | 21.8M/32.4M [00:05<00:04, 2.17MB/s]dev.tgz:  75%|███████▍  | 24.2M/32.4M [00:05<00:02, 2.80MB/s]dev.tgz:  82%|████████▏ | 26.5M/32.4M [00:05<00:01, 3.50MB/s]dev.tgz:  89%|████████▉ | 28.9M/32.4M [00:05<00:00, 4.26MB/s]dev.tgz:  97%|█████████▋| 31.4M/32.4M [00:06<00:00, 5.03MB/s]dev.tgz: 32.4MB [00:06, 5.21MB/s]                            
[*] Extracting dev.tgz.
/home/joonsun/nmt-practice-pytorch/raw_data
[*] Downloading from https://storage.googleapis.com/tf-perf-public/official_transformer/test_data/newstest2014.tgz to newstest2014.tgz.
newstest2014.tgz: 0.00B [00:00, ?B/s]newstest2014.tgz:   3%|▎         | 8.19k/298k [00:00<00:16, 17.1kB/s]newstest2014.tgz:  14%|█▍        | 41.0k/298k [00:00<00:10, 23.9kB/s]newstest2014.tgz:  39%|███▊      | 115k/298k [00:00<00:05, 33.6kB/s] newstest2014.tgz: 303kB [00:00, 391kB/s]                            
[*] Extracting newstest2014.tgz.
[*] Merge files into two files: /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.src and /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.trg.
  Input files: 
    - SRC: /home/joonsun/nmt-practice-pytorch/raw_data/training-parallel-nc-v13/news-commentary-v13.de-en.de
    - TRG: /home/joonsun/nmt-practice-pytorch/raw_data/training-parallel-nc-v13/news-commentary-v13.de-en.en
[*] Merge files into two files: /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-val.src and /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-val.trg.
  Input files: 
    - SRC: /home/joonsun/nmt-practice-pytorch/raw_data/dev/newstest2013.de
    - TRG: /home/joonsun/nmt-practice-pytorch/raw_data/dev/newstest2013.en
[*] Merge files into two files: /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-test.src and /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-test.trg.
  Input files: 
    - SRC: /home/joonsun/nmt-practice-pytorch/raw_data/newstest2014.de
    - TRG: /home/joonsun/nmt-practice-pytorch/raw_data/newstest2014.en
[*] Training Sentence Piece model: sp-de-en...
sentencepiece_trainer.cc(179) LOG(INFO) Running command: --input=/home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.trg,/home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.src --model_prefix=sp-de-en --vocab_size=32000 --bos_id=-1 --eos_id=-1 --character_coverage=1.0 --model_type=unigram --normalization_rule_name=nfkc_cf
sentencepiece_trainer.cc(79) LOG(INFO) Starts training with : 
trainer_spec {
  input: /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.trg
  input: /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.src
  input_format: 
  model_prefix: sp-de-en
  model_type: UNIGRAM
  vocab_size: 32000
  self_test_sample_size: 0
  character_coverage: 1
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  treat_whitespace_as_suffix: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: -1
  eos_id: -1
  pad_id: -1
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
}
normalizer_spec {
  name: nfkc_cf
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(320) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(175) LOG(INFO) Loading corpus: /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.trg
trainer_interface.cc(175) LOG(INFO) Loading corpus: /home/joonsun/nmt-practice-pytorch/raw_data/raw-de-en-train.src
trainer_interface.cc(376) LOG(INFO) Loaded all 568492 sentences
trainer_interface.cc(391) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(396) LOG(INFO) Normalizing sentences...
trainer_interface.cc(457) LOG(INFO) all chars count=86566669
trainer_interface.cc(468) LOG(INFO) Done: 100% characters are covered.
trainer_interface.cc(478) LOG(INFO) Alphabet size=235
trainer_interface.cc(479) LOG(INFO) Final character coverage=1
trainer_interface.cc(511) LOG(INFO) Done! preprocessed 568492 sentences.
unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...
unigram_model_trainer.cc(193) LOG(INFO) Initialized 717422 seed sentencepieces
trainer_interface.cc(517) LOG(INFO) Tokenizing input sentences with whitespace: 568492
trainer_interface.cc(527) LOG(INFO) Done! 426495
unigram_model_trainer.cc(488) LOG(INFO) Using 426495 sentences for EM training
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=177207 obj=11.955 num_tokens=915448 num_tokens/piece=5.16598
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=143271 obj=9.24564 num_tokens=915774 num_tokens/piece=6.3919
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=107415 obj=9.22402 num_tokens=956782 num_tokens/piece=8.90734
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=107344 obj=9.21431 num_tokens=957943 num_tokens/piece=8.92405
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=80503 obj=9.26719 num_tokens=1022269 num_tokens/piece=12.6985
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=80499 obj=9.25653 num_tokens=1022332 num_tokens/piece=12.6999
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=60371 obj=9.33865 num_tokens=1096990 num_tokens/piece=18.1708
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=60369 obj=9.32392 num_tokens=1096865 num_tokens/piece=18.1693
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=45276 obj=9.43948 num_tokens=1177826 num_tokens/piece=26.0144
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=45276 obj=9.41924 num_tokens=1177824 num_tokens/piece=26.0143
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=35200 obj=9.5501 num_tokens=1252968 num_tokens/piece=35.5957
unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=35199 obj=9.52727 num_tokens=1252986 num_tokens/piece=35.5972
trainer_interface.cc(605) LOG(INFO) Saving model: sp-de-en.model
trainer_interface.cc(616) LOG(INFO) Saving vocabs: sp-de-en.vocab
[*] Preparing vocabulary...
	[-] The vocabularies are shared.
[*] Preparing dataset...
[*] Building the vocabularies...
[*] Dumping the processed data to data directory /home/joonsun/nmt-practice-pytorch/data
	Saving training dataset...
	Saving validation dataset...
	Saving test dataset...


[*] RESULTS:
	 283733 training data 2992 validation data and 3001 test data were preprocessed.
	 31999 source vocabulary and 31999 target vocabulary were saved.
